#!/usr/bin/env ruby

require 'benchmark'
require 'opener/build-tools'
require 'opener/language_identifier'
require 'opener/tokenizer'
require 'opener/pos_tagger'
require 'opener/polarity_tagger'
require 'opener/opinion_detector'

include Opener::BuildTools::Requirements
include Opener::BuildTools::Python
include Opener::BuildTools::Perl
include Opener::BuildTools::Java

##
# @param [String] text
# @param [Numeric] label_width
# @return [String]
#
def benchmark_header(text, label_width)
  text = text + ' '

  return text.ljust(label_width + 1 + Benchmark::CAPTION.length, '-')
end

##
# @param [Class] constant
# @param [String] input
# @param [Hash] options
# @return [String]
#
def run_component(constant, input, options = {})
  runner                  = constant.new(options)
  stdout, stderr, process = runner.run(input)

  if process.success?
    return stdout.strip
  else
    raise "Failed to run the component #{constant}: #{stderr}"
  end
end

fixture    = File.expand_path('../../fixtures/reddit_comment.txt', __FILE__)
input      = File.read(fixture)
iterations = 10
width      = 30

puts benchmark_header('Requirements', width)
puts

require_executable('python')
require_version('python', python_version, '2.7')

require_executable('perl')
require_version('perl', perl_version, '5.10')

require_executable('java')
require_version('java', java_version, '1.7')

puts <<-EOF

#{benchmark_header('Setup', width)}

Input text: #{fixture}
iterations: #{iterations}

Platform: #{`uname -s -r -v -m -o 2>&1`.strip}
Ruby:     #{RUBY_VERSION}
Perl:     #{Opener::BuildTools::Perl.perl_version}
Python:   #{Opener::BuildTools::Python.python_version}
Java:     #{Opener::BuildTools::Java.java_version}

Each benchmark is ran twice so that potential "warm ups" can be taken into
account. Each benchmark contains #{iterations} iterations of the specified code
to benchmark.

Collecting benchmarking data...

EOF

# Pre-collect this data so that a benchmark doesn't also have to benchmark the
# previous step.
language = run_component(Opener::LanguageIdentifier, input, :kaf => true)
tokens   = run_component(Opener::Tokenizer, language, :kaf => true)
pos      = run_component(Opener::POSTagger, tokens)
polarity = run_component(Opener::PolarityTagger, pos)

timings = Benchmark.bmbm(width) do |bench|
  bench.report 'language-identifier' do
    iterations.times do
      run_component(Opener::LanguageIdentifier, input, :kaf => true)
    end
  end

  bench.report 'tokenizer' do
    iterations.times do
      run_component(Opener::Tokenizer, language, :kaf => true)
    end
  end

  bench.report 'pos-tagger' do
    iterations.times do
      run_component(Opener::POSTagger, tokens)
    end
  end

  bench.report 'polarity-tagger' do
    iterations.times do
      run_component(Opener::PolarityTagger, pos)
    end
  end

  bench.report 'opinion-detector' do
    iterations.times do
      run_component(Opener::OpinionDetector, polarity)
    end
  end
end

# Display the amount of time it took for a single iteration.
timing_width = timings.map(&:label).sort { |a, b| b.length <=> a.length }
timing_width = timing_width[0].length + 5

puts
puts benchmark_header('Time per iteration', width)
puts

timings.each do |timing|
  print "#{timing.label}:".ljust(timing_width, ' ')

  time = (timing.total / iterations).round(3)

  print "#{time}sec"
  puts
end
